
\chapter{医学影像与深度学习相关知识}
\label{chap:denoising}

我们提出了一个完全卷积编码器 - 解码器框架的图像残差变换任务。 所提出的框架不是仅使用每像素丢失函数，而是结合依赖于来自预先训练的网络的低级特征的感知损失函数来学习端到端映射。 通过引入身份映射指出映射函数以处理无噪声图像。 并通过分析神经网络与他们试图学习的基本噪声分布之间的相互作用。 我们还展示了如何构建一个统一的变换，然后使用这个统一的变换使单个深度神经网络能够在不同的噪声级别上正常工作。 与以前的方法相比，我们的性能更好。 实验结果表明了该算法处理图像去噪任务的有效性。

\section{医学影像}

 
\section{人工神经网络}

 
\section{卷积神经网络}

深度CNN是多层前馈神经网络的一种特例。隐藏层的神经元设计成跟上一层神经元局部连接，并利用参数共享来减少模型复杂度。针对图像这种结构化数据，由不同卷积核来探测不同空间位置上的局部统计特征。通过堆叠多层的卷积结构，实现从低层到高层语义空间的抽象映射。
深度CNN的典型结构是在LeNet模型\citep{Jarrett2009}的基础上引入修正线性单元(Rectified Linear Units,ReLU)的激活函数和Dropout等技术\citep{Krizhevsky2012}进行了改进。\ 为CNN模型的网络结构示意图。定义图像数据为 ，且其类别标签 ，其中 和 ，k为类别数， 作为网络输入，输入层的 ，即原始图像作为输入，第 层输出 个大小为 的特征图。第一层为由  个特征图作为输入的卷积层，特征图大小为  。第 层第 特征图定义为  。计算公式为:
      (1)
其中 为偏置矩阵， 为连接第 层第 个特征图和第 层第 个特征的卷积核。
模型的激活函数没有采用Sigmoid函数或双曲正切函数，而是选择ReLU函数，目的是引入更多非线性来加速训练收敛速度，解决多层网络反向传播中梯度弥散的问题。其函数表达式为： 
              (2)
          (3)

 其中 表示对第 层的激活函数，该层一般嵌入在卷积层后。为了使得每层输入的分布更平稳，一般引入批量归一化层（Batch Normalization， BN），如图1中所示。最大池化层进行下采样，有时把“卷积-激活-归一化-池化”统称为卷积层。最后需连接全连接层（图中Fc层表示），全连接层就不再保存空间信息，是对低层特征的高层抽象，最终输出K维的向量，作为该图像的特征向量送入最终的分类器进行分类评估。

\subsection{全卷积网络} 
图 1 卷积网络模型结构示意图
Fig.1 The structure of convolutions model
深度CNN模型的分类器与传统方法不同的是：把特征提取过程中的卷积核参数和分类器的参数整合到端到端的模型中。对一个有监督的多分类问题，特征提取过程可表示为得分函数 ，W，b是各层可学习的参数包括卷积核K，偏置B和全连接层的权值参数。对第 个样本的得分函数分类误差的交叉熵损失函数可定义为：
  	 (4)
     (5)
通过最小化Softmax函数的非负对数似然（公式5），能带来归一化的概率解释。一般采用L2损失正则化技术提升分类泛化性能。全部N个样本的损失函数L为公式6所示。其中  表示正则化参数。模型最小化方法采用反向传播算法，通过带动量的批随机梯度下降算法不断调整参数使得模型整体误差函数不断降低。并通过使用权重衰减项和Dropout技术控制过拟合。具体实现详情请参考文献[10]。

\subsection{全卷积网络}
 
\section{深度卷积神经网络}
\section{本章小结}


